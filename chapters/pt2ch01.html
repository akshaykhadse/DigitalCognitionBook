<!DOCTYPE html>
<html lang="en-us">

  <head>
    <link href="https://gmpg.org/xfn/11" rel="profile" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  
    <!-- Enable responsiveness on mobile devices-->
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  
    <title>
      
        Articles
      
    </title>
  
    
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-175939743-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-175939743-1');
</script>


  
    <!-- CSS -->
    <link rel="stylesheet" href="/DigitalCognitionBook/assets/css/main.css" />
    

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface" />
  
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/DigitalCognitionBook/favicon.png" />
<link rel="shortcut icon" href="/DigitalCognitionBook/favicon.ico" />
  
    <!-- RSS -->
    <link rel="alternate" type="application/rss+xml" title="RSS" href="/DigitalCognitionBook/feed.xml" />
  
    <!-- Additional head bits without overriding original head -->
<link rel="stylesheet" href="/DigitalCognitionBook/assets/css/prism.css">
  </head>

  <body class="chapter">

    <div id="sidebar">
  <header>
    <div class="site-title">
      <a href="/DigitalCognitionBook/">
        
          <span class="back-arrow icon"><svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
  <path d="M0 0h24v24H0z" fill="none"/>
  <path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
</svg></span>
        
        Digital Cognition
      </a>
    </div>
    <p class="lead">An Incomplete Guide</p>
  </header>
  <nav id="sidebar-nav-links">
  
  

  

  


  
    
  

  

  
    
  

  

  

  

  


  


  
    
  

  

  
    
  

  

  

  

  


  <!-- Optional additional links to insert in sidebar nav -->



  
    
      <a class="category-link "
          href="/DigitalCognitionBook/parts/part1.html">Linear Algebra</a>
    
  

  
    
      <a class="category-link  active"
          href="/DigitalCognitionBook/parts/part2.html">Statistics</a>
    
  

  
    
      <a class="category-link "
          href="/DigitalCognitionBook/parts/part3.html">Machine Learning</a>
    
  

</nav>


  

  
  <!--
  <p>
    &copy; 2021.
    <a href="/DigitalCognitionBook/LICENSE.md">MIT License.</a>
  </p>
-->

</div>

    <main class="container">
      <header>
  <h1 class="page-title">Chapter 1: Articles</h1>
</header>
<div class="content">
  
<h2><a name="pt2ch1-BetaDistribution"></a>Beta Distribution</h2>
        <p>The beta distribution is a family of continuous probability distributions defined on the interval \([0, 1]\) parameterized by two positive shape parameters, denoted by \(\alpha\) and \(\beta\), that control the shape of the distribution.</p>

<p>This is denoted as \(X \sim \operatorname{Beta}(\alpha, \beta)\).</p>

<p>The probability density function of Beta distribution is given by</p>

\[f(x \mid \alpha, \beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\mathcal{B}(\alpha, \beta)}\]

<p>Here, \(\mathcal{B}(\alpha, \beta)\) is called the Beta function and it is given by</p>

\[\begin{align}
\mathcal{B}(\alpha,\beta) &amp;= \int_{0}^{1} x^{\alpha-1}(1-x)^{\beta -1} dx\\
&amp;= \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}
\end{align}\]

<p>where, \(\Gamma(n) = (n-1)!\) for all \(n \in \mathbb{Z}^{+}\).</p>

<p>Let’s see the effect of \(\beta\) on the PDF
<img src="/DigitalCognitionBook/assets/images/beta_dustribution_beta.gif" alt="Beta Distribution" width="80%" height="80%" /></p>

<p>The effect of \(\alpha\) and \(\beta\) on the PDF is summarized in the following image
<img src="/DigitalCognitionBook/assets/images/beta_distribution.svg" alt="Beta Distribution" width="80%" height="80%" /></p>
<h2><a name="pt2ch1-MaximumLikelihoodEstimate"></a>Maximum Likelihood Estimate</h2>
        <p>Suresh tosses a coin 1000 times and he observes heads 400 times.</p>

<p>This is a stochastic process that takes discrete values viz. heads (1) or tails 
(0). In such cases, we can calculate the probability of observing a particular 
set of outcomes by making suitable assumptions about the underlying stochastic 
process. For example, in our case, probability of coin landing heads is \(p\) 
and that coin tosses are independent.</p>

<p>Let’s denote the observed outcomes by \(O\) and the set of parameters that 
describe the stochastic process as \(\theta\). Our observed outcome is 400 
heads and 600 tails. The process of tossing a coin is governed only by the 
probability \(p\). Hence, \(\theta = p\). Thus, when we speak of probability 
we want to calculate \(P(O \mid \theta)\). In other words, given specific 
values for \(\theta\), \(P(O \mid \theta)\) is the probability that we would 
observe the outcomes represented by \(O\).</p>

<p>However, when we model a real life stochastic process, we often do not know 
\(\theta\). We simply observe \(O\) and the goal then is to arrive at an 
estimate for \(\theta\) that would be a plausible choice given the observed 
outcomes \(O\). We know that given a value of \(\theta\) the probability of 
observing \(O\) is \(P(O \mid \theta)\). Thus, a ‘natural’ estimation process 
is to choose that value of \(\theta\) that would maximize the probability that 
we would actually observe \(O\). In other words, we find the parameter values 
\(\theta\) that maximize the following function:</p>

\[L(\theta \mid O) = P(O \mid \theta)\]

<p>\(L(\theta \mid O)\) is called the likelihood function. Notice that by 
definition the likelihood function is conditioned on the observed \(O\) and 
that it is a function of the unknown parameters \(\theta\).</p>

<p>Suresh is about to toss the coin again and he would like to know the 
probability of getting a heads in this toss. We can estimate the probability 
of observing heads as</p>

\[P(Heads) = \frac{400}{1000} = 0.4 = \hat{\theta}_{\tiny{MLE}}\]

<p>However, it is important to note that we do not know the exact probability of 
getting the heads \(\theta\). \(\hat{\theta}_{\tiny{MLE}}\) is an estimate 
that maximises the likelihood of observing the outcomes that we already saw. 
In other words, it the value of the parameter which is the most reasonable 
explantion for our observations.</p>

<p>It’s all good for argument sake, but can we prove this theoritically?</p>

<p>We know that tossing a coin 1000 times is a Bernouli trial and the number of 
heads (denoted by random variable \(X\)) follows a Binomial distribution. 
Let’s assume that \(\theta\) is the probablity of getting a head in any single 
coin toss. Thus, probability of getting a tails in single toss will be 
\((1-\theta)\). Hence, the probability of getting \(k\) heads in \(n\) coin 
tosses is given by</p>

\[P(X = k \mid \theta) = {}^{n}C_{k} \; \theta^{k}(1-\theta)^{n-k}\]

<p>where</p>

<p>Now, we can formulate our maximum likelihood estimate as</p>

\[\begin{align}
\hat{\theta}_{\tiny{MLE}} &amp;= \underset{\theta}{\operatorname{argmax}} L(\theta \mid X = k)\\
&amp;=  \underset{\theta}{\operatorname{argmax}} P(X = k \mid \theta)\\
&amp;=  \underset{\theta \in [0,1]}{\operatorname{argmax}} {}^{n}C_{k} \; \theta^{k}(1-\theta)^{n-k}
\end{align}\]

<p>Instead of maximizing the likelihood, we will maximise the log of likelihood. 
Since log is a monotonically increasing function, it preserves the location of 
maximas and minimas of the original function</p>

\[\begin{align}
LL(\theta \mid X = k) &amp;= \log (L(\theta \mid X = k))\\
&amp;= \log ({}^{n}C_{k} \; \theta^{k}(1-\theta)^{n-k})\\
&amp;= \log ({}^{n}C_{k}) + k \log(\theta) + (n-k) \log(1-\theta)
\end{align}\]

<p>Therefore,</p>

\[\begin{align}
\hat{\theta}_{\tiny{MLE}} &amp;= \underset{\theta}{\operatorname{argmax}} L(\theta \mid X = k)\\
&amp;=  \underset{\theta}{\operatorname{argmax}} LL(\theta \mid X = k)\\
&amp;=  \underset{\theta \in [0,1]}{\operatorname{argmax}} \log ({}^{n}C_{k}) + k \log(\theta) + (n-k) \log(1-\theta)
\end{align}\]

<p>Differentiating with respect to \(\theta\),</p>

\[\begin{align}
\frac{d}{d \theta} LL(\theta \mid X = k) &amp;= \frac{d}{d \theta} \log ({}^{n}C_{k}) + k \frac{d}{d \theta} \log(\theta) + (n-k) \frac{d}{d \theta} \log(1-\theta)\\
&amp;= \frac{k}{\theta} - \frac{n-k}{1-\theta}
\end{align}\]

<p>For \(\theta\) to be maximum,</p>

\[\frac{d}{d \theta} LL(\theta \mid X = k) = 0\\
\frac{k}{\theta} - \frac{n-k}{1-\theta} = 0\\
\frac{k}{\theta} = \frac{n-k}{1-\theta}\\
\theta = \frac{k}{n}\]

<p>Thus, the maximum likelihood estimator for \(\theta\) is</p>

\[\hat{\theta}_{\tiny{MLE}} = \frac{k}{n}\]

<p>This is the same estimate which we calculated initially i.e. \(\hat{\theta}_{\tiny{MLE}} = \frac{400}{1000} = 0.4\)</p>

<h3 id="continuous-random-variables">Continuous Random Variables</h3>

<p>If the stochatistc process has a continous random variable as its outcome, 
then the situation is similar, with one important difference. We can no longer 
talk about the probability that we observed \(O\) given \(\theta\) because in 
the continuous case \(P(O \mid \theta)=0\). Without getting into 
technicalities, the basic idea is as follows:</p>

<p>Denote the probability density function (pdf) associated with the outcomes 
\(O\) as: \(f(O \mid \theta)\). Thus, in the continuous case we estimate 
\(\theta\) given observed outcomes \(O\) by maximizing the following function:</p>

\[L(\theta \mid O)=f(O \mid \theta)\]

<p>Our use of the pdf instead of actual probability intutuively makes sense 
because if we compare probabilities of getting a small set of outcome e.g. 
\(P(300 \leq O \leq 301 \mid \theta)\) instead of a particular outcome e.g. 
\(P(O = 300 \mid \theta)\), then we can see that the probabilities are 
actually proportional to the value of pdf 
\(f((O_{max} + O_{min})/2 \mid \theta)\).</p>

<p>In this situation, we cannot technically assert that we are finding the 
parameter value that maximizes the probability that we observe \(O\) as we 
maximize the PDF associated with the observed outcomes \(O\).</p>

<h3 id="derivation-of-mle">Derivation of MLE</h3>
<p>Suppose we observe \(n\) samples. Let each sample be denoted by random 
variables \(X_i\), \(X_2\), …, \(X_n\). Its safe to assume that \(X_i\)’s 
are independent of each other i.e drawing a sample from the population does 
not affect the chances of getting any other sample. Each random variable is 
governed by a distribution which follows the probability mass function (in 
discrete random variables) or probability density function (in case of 
continous random variabes) \(f_{\theta}(x_i) = f(x_i \mid \theta)\). This 
means that the distribution is governed by a parameter \(\theta\). Such 
random variables are said to be independent and identically distributed 
(i.i.d).</p>

<p>Now, if we observe that \(X_i = x_i\), \(X_2 = x_2\), …, \(X_n = x_n\). 
Then, the probability of making this observation is given by a function 
\(f_{\theta}(x_1, x_2, ..., x_n)\). This is known as joint probability mass 
function (for discrete random variables) and probability mass function (for 
continous random variables). It is given as</p>

\[\begin{align}
f_{\theta}(x_1, x_2, ..., x_n) &amp;= f(x_1, x_2, ..., x_n \mid \theta)\\
&amp;= \prod^{i=1}{n} f_{theta}(x_i)\\
\end{align}\]

<p>The likelihood function of \(\theta\) is given by</p>

\[\begin{align}
L(\theta \mid \mathcal{D}) &amp;= L(\theta \mid x_1, x_2, ..., x_n)\\
&amp;= f_{\theta}(x_1, x_2, ..., x_n)\\
&amp;= f(x_1, x_2, ..., x_n \mid \theta)\\
&amp;= \prod^{i=1}{n} f_{theta}(x_i)
\end{align}\]

<p>Thus the maximum likihood extimator of \(\theta\) is given by</p>

\[\begin{align}
\hat{\theta}_{\tiny{MLE}} &amp;= \underset{\theta}{\operatorname{argmax}} L(\theta \mid \mathcal{D})\\
&amp;= \underset{\theta}{\operatorname{argmax}} \prod^{i=1}{n} f_{theta}(x_i)
\end{align}\]
<h2><a name="pt2ch1-Entropy"></a>Entropy</h2>
        <p>The entropy of a random variable is measure of its uncertainity. Some texts 
also refer it as a measure of information content.</p>

<p>But, what is the meaning of uncertainity or information content? Let’s 
understand with an example
<img src="/DigitalCognitionBook/assets/images/entropy_example.svg" alt="Entropy example" width="80%" height="80%" />
We have two random alphabet generator machines which generate one character 
from the set \(\{A, B, C, D\}\). The probabilities of generating each alphabet 
is, however, different for both machines. For machine 1, each alphabet is 
equaly likely to be generated i.e. \(P(A) = P(B) = P(C) = P(D) = \frac{1}{4}\). 
However, for machine 2, probabilities of getting each alphabet is 
\(P(A) = \frac{1}{2}\), \(P(B) = \frac{1}{4}\), \(P(C) = \frac{1}{8}\) and 
\(P(D) = \frac{1}{8}\).</p>

<p>But, you cannot directly generate a alphabet from these machines. As it turns 
out, these machines have overly complicated controls for a seemingly simple 
task. Hence, there is a machine operator who can get the machine to generate a 
alphabet for you. Machine operator generates an alphabet from the machine but 
insists that you guess the alphabet correctly by asking some questions only 
then would he give the alphabet to you. Given the absurd situation you have 
found yourselves in, you would like to get over with it by asking minimum 
number of questions to the operator.</p>

<p>Consider the case where you ask operator to generate an aplhabet from machine 
1.
<img src="/DigitalCognitionBook/assets/images/entropy_machine1.svg" alt="Questions machine 1" width="50%" height="50%" />
You ask the first question to operator, “Is the alphabet A or B?”. The 
operator says yes. Then you ask, “Is the alphabet A?”. The operator respond 
no. Hence, you tell the operator that the machine generated the alphabet B. 
The operator then hands it over to you.</p>

<p>Please note that to guess the alphabet correctly, you had to ask two 
questions. From the decision tree in the above image, you can see that even if 
the operator responded differently for any of the questions, you would still 
need to ask two questions. Thus, if you repeat this experiment for 100 times, 
you would need to ask 200 questions i.e. on an average 2 questions per trial.</p>

<p>Let’s see what happens for machine 2. Since, probability of getting A is 
highest, you should start asking questions from A and then move to alphabets 
with lower probability of occurrence. This will ensure that if you repeat the 
trial sevel times, you will be asking minimum number of question.
<img src="/DigitalCognitionBook/assets/images/entropy_machine2.svg" alt="Questions machine 2" width="50%" height="50%" />
You ask the operator to get a new alphabet from the machine and start with a 
question “Is the alphabet A?”. The operator responds with a no. Then, you ask, 
“Is the alphabet B?”. The operator says yes. The operator hands the alphabet 
over to you as you guessed it correctly.</p>

<p>Note that you had to ask two questions to correctly guess the alphabet B. 
However, you can see that in the decision tree, you would need to ask just one 
question if the alphabet generated by the machine 2 was A. Similarly, you 
would need to ask three questions to correctly guess C or D.</p>

<p>If you repeat this experiment 800 times, you can expect to see 400 A’s, 200 
B’s, 100 C’s and 100 D’s. Thus the average number of questions that you would 
need to ask is 
\(\frac{400 \cdot 1 + 200 \cdot 2 + 100 \cdot 3 + 100 \cdot 3}{800} = 1.75\).</p>

<p>The more uncertain the outcome of an experiment is, more number of questions 
you would ask to guess the outcome correctly. Thus the average number of 
questions you ask is the measure of uncertainity. This can also be termed as 
information content. If the probability of occurence of an event is \(p\) then 
you would more or less (not exactly always) be asking \(-\log_{2}(p)\) 
questions. So, the average number of questions that you would be asking is 
entropy.</p>

<p>Given a discrete random variable \(X\), with possible outcomes \(x_1\), …, 
\(x_n\), which occur with probability \(P(x_1)\), …, \(P(x_n)\), the entropy 
of \(X\) is formally defined as:</p>

\[H(X) = - \sum_{i=1}^{n} P(x_i) \log_{2}(P(x_i))\]

<p>Of all the distributions with variance \(\sigma^2\), Gaussian distribution has 
the highest entropy. It models the highest uncertainity. Hence, it usually 
chosen to model unknown random variables as it encapsulates the worst case 
scenario of maximum uncertainity. This is the reason we chose 
\(\mathcal{N}(0, \sigma^2)\) to model the error \(\varepsilon\).</p>
<h2><a name="pt2ch1-MaximumA-PosterioriEstimate"></a>Maximum A-Posteriori Estimate</h2>
        <p>Consider the exmaple of coin toss which we saw earlier. Let’s say Suresh gets himself a new coin from the mint. The coin has one face heads and other tails. Since coin is brand new, he believes that the coin is fair. So, getting heads and tails is equally likely i.e. \(P(H) = P(T) = 0.5\).</p>

<p>Now, Suresh tosses the coins four times and he obeserves heads all four times. What is the porbability of getting heads in the next toss? Well, if we were looking at the maximum likelihood estimate, then the answer would be \(\hat{P}(H) = 4/4 = 1\).</p>

<p>Does this mean that tails is improbable? We know that it would be practically possible to get heads all the time only if both the faces of the coin were heads (and we were in the movie Sholay). This is clearly not true for our coin, it has heads on one face and tails on the other. This means that Suresh’s belif that the coin is fair is not correct. However, it does not mean that the tails is improbabe. So, is there a way through which Suresh can update his belif that \(\hat{p} = 0.5\) by incorpating the information he recieved by tossing coin four times?</p>

<p>Yes. Let me introduce you to Bayesian Inference. Let’s say that the \(X\) is governed by a distribution whose parameter is \(\theta\). Under the Bayesian Inference, you assume that \(\theta\) in itself is another random variable. The you can incorporate your belif about \(\theta\) via assuming a probability distribution that governs \(\theta\). We will represent this distribution via a PDF or PMF \(g(\theta)\). The probability distribution of \(\theta\) which incorporates your belif \(g(\theta)\) is called prior because this denotes your belif about \(\theta\) prior to observing evidence or data \(\mathcal{D}\).</p>

<p>As we saw with Suresh’s coin toss example, we may encounter some evidence or data \(\mathcal{D}\) which can be used to update our original belif. This evidence can be encorporated in a distribution denoted by \(f(x \mid \theta)\). No fancy names for this one.</p>

<p>Then we can conclude the following from the Bayes’ rule</p>

\[f(\theta \mid x) = \frac{f(x \mid \theta) g(\theta)}{\int_{\Theta} f(x \mid t) g(t) dt}\]

<p>where \(\Theta\) is the domain of function \(g(\theta)\). The set of values over which a function is defined is called domain of that function. In other words, \(\Theta\) deontes range of \(\theta\) over which function \(g(\theta)\) is defined.</p>

<p>In this equation, the denominator does not depend on the \(\theta\). Therefore, it is constant. We can see that maximizing \(f(\theta \mid x)\) is same as maximizing \(f(x \mid \theta) g(\theta)\). Thus we can write</p>

\[f(\theta \mid x) \propto f(x \mid \theta) g(\theta)\]

<p>In this equation \(f(\theta \mid x)\) is known as posterior distribution of \(\theta\).</p>

<p>The maximum a-posteriori estimate is the value of \(\theta\) which maximises the posterior distribution of \(\theta\). This is given as</p>

\[\begin{align}
\hat{\theta}_{MAP} &amp;= \underset{\theta}{\operatorname{argmax}} f(\theta \mid x)\\
&amp;= \underset{\theta}{\operatorname{argmax}} f(x \mid \theta) g(\theta)
\end{align}\]

<p>Observe that the MAP estimate of \(\theta\)  coincides with the ML estimate when the prior \(g\) is uniform (that is, a constant function).</p>

<p>Let’s get back to Suresh. Coin toss is an example of Binomial distribution. Let probability of getting heads in a single coin toss be \(p\). The probablity of observing \(h\) heads in \(n\) tosses is given by</p>

\[f(h \mid p) = {}^{n}C_{h} p^h (1-p)^{n-h}\]

<p>Assume that \(p\) is also a random variable. We will model the prior of \(p\) as a Beta distribution (beacuse Beta is the conjugate prior of Binomial. This will be discussed later when explaining conjugate priors) with parameter \(\alpha = 10\) and \(\beta = 10\). This is shown in the following figure</p>

<p><img src="/DigitalCognitionBook/assets/images/beta_distribution_10_10.png" alt="" width="80%" height="80%" /></p>

<p>Observe that this mound shaped curve has peak at \(x = 0.5\) and looks quite similar to the Gassian distribution. Thus, this distribution incorporates the Suresh’s belief that the coin is fair. Thus,</p>

\[g(p) = \frac{p^{\alpha-1} (1-p)^{\beta-1}}{\mathcal{B}(\alpha,\beta)}\]

<p>Now, the posterior distribution \(f(p \mid x)\) is given by</p>

\[\begin{align}
f(p \mid h) &amp;= \frac{f(h \mid p) g(p)}{ \int_{0}^{1} f(h \mid p) g(p) dp} \\
&amp;= \frac{ {}^{n}C_{h} p^h (1-p)^{n-h} \times \frac{p^{\alpha-1} (1-p)^{\beta-1}}{\mathcal{B}(\alpha,\beta)}}{\displaystyle \int_{0}^{1} {}^{n}C_{h} p^h (1-p)^{n-h} \times \frac{p^{\alpha-1} (1-p)^{\beta-1}}{\mathcal{B}(\alpha,\beta)} dp}\\
&amp;= \frac{p^h (1-p)^{n-h} \times p^{\alpha-1} (1-p)^{\beta-1}} {\displaystyle \int_{0}^{1} p^h (1-p)^{n-h} \times p^{\alpha-1} (1-p)^{\beta-1} dp}\\
&amp;= \frac{p^{\alpha + h - 1} (1-p)^{\beta + n - h -1}} {\displaystyle \int_{0}^{1} p^{\alpha + h - 1} (1-p)^{\beta + n - h -1} dp} \\
&amp;= \frac{p^{\alpha + h - 1} (1-p)^{\beta + n - h -1}} {\mathcal{B}(\alpha + h, \beta + n - h)} \\
&amp;= \operatorname{Beta}(\alpha + h, \beta + n - h)
\end{align}\]

<p>Now, the Maximum A-Posteriori estimate is,</p>

\[\begin{align}
\hat{p}_{\tiny{MAP}} &amp;= \underset{p}{\operatorname{argmax}} f(p \mid h)\\
&amp;= \underset{p}{\operatorname{argmax}} \frac{p^{\alpha + h - 1} (1-p)^{\beta + n - h -1}} {\mathcal{B}(\alpha + h, \beta + n - h)}\\
&amp;= \operatorname{mode}(\operatorname{Beta}(\alpha + h, \beta + n - h))\\
&amp;= \frac{\alpha + h -1}{\alpha + \beta + n -2}
\end{align}\]

<p>So, for Suresh, the MAP estimate of probability is \(\frac{10+4-1}{10+10+4-2} = 0.6\)</p>

<p>Please notice that the probabilty of getting heads is 0.6 as opposed to 1 which we found out from MLE. MAP estimate is much more reasonable than the MLE and it incorporates our original belif that the coin is fair. This also explains that the tails is not improbable.</p>
</div>

    </main>

    <!-- Optional footer content -->
<script src="/DigitalCognitionBook/assets/js/prism.js"></script>
<script src="/DigitalCognitionBook/plugins/normalize-whitespace/prism-normalize-whitespace.js></script>
<script type="text/javascript">
Prism.plugins.NormalizeWhitespace.setDefaults({
'remove-trailing': true,
'remove-indent': false,
'left-trim': false,
'right-trim': false,
});
</script>
<script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 90 }
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  </body>
</html>
