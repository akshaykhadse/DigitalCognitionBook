<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

  <title>
    
      Linear Regression &middot; Digital Cognition
    
  </title>

  


  <!-- CSS -->
  <link rel="stylesheet" href="/DigitalCognitionBook/assets/css/main.css" />
  

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface" />

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/DigitalCognitionBook/favicon.png" />
<link rel="shortcut icon" href="/DigitalCognitionBook/favicon.ico" />

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml" />

  <!-- Additional head bits without overriding original head -->
<link rel="stylesheet" href="/DigitalCognitionBook/assets/css/prism.css">
</head>


  <body class="chapter">

    <div id="sidebar">
  <header>
    <div class="site-title">
      <a href="/DigitalCognitionBook/">
        
          <span class="back-arrow icon"><svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
  <path d="M0 0h24v24H0z" fill="none"/>
  <path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
</svg></span>
        
        Digital Cognition
      </a>
    </div>
    <p class="lead">An Incomplete Guide</p>
  </header>
  <nav id="sidebar-nav-links">
  
    <a class="home-link "
        href="/DigitalCognitionBook/">Home</a>
  
  

  

  


  
    
  

  

  
    
  

  

  

  

  


  


  
    
  

  

  
    
  

  

  

  

  


  <!-- Optional additional links to insert in sidebar nav -->
</nav>


  

  <nav id="sidebar-icon-links">
  
    <a id="github-link"
       class="icon" title="Github Project" aria-label="Github Project"
       href="https://github.com/akshaykhadse/DigitalCognitionBook">
      <svg version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 28" height="24" width="28"><path d="M12 2c6.625 0 12 5.375 12 12 0 5.297-3.437 9.797-8.203 11.391-0.609 0.109-0.828-0.266-0.828-0.578 0-0.391 0.016-1.687 0.016-3.297 0-1.125-0.375-1.844-0.812-2.219 2.672-0.297 5.484-1.313 5.484-5.922 0-1.313-0.469-2.375-1.234-3.219 0.125-0.313 0.531-1.531-0.125-3.187-1-0.313-3.297 1.234-3.297 1.234-0.953-0.266-1.984-0.406-3-0.406s-2.047 0.141-3 0.406c0 0-2.297-1.547-3.297-1.234-0.656 1.656-0.25 2.875-0.125 3.187-0.766 0.844-1.234 1.906-1.234 3.219 0 4.594 2.797 5.625 5.469 5.922-0.344 0.313-0.656 0.844-0.766 1.609-0.688 0.313-2.438 0.844-3.484-1-0.656-1.141-1.844-1.234-1.844-1.234-1.172-0.016-0.078 0.734-0.078 0.734 0.781 0.359 1.328 1.75 1.328 1.75 0.703 2.141 4.047 1.422 4.047 1.422 0 1 0.016 1.937 0.016 2.234 0 0.313-0.219 0.688-0.828 0.578-4.766-1.594-8.203-6.094-8.203-11.391 0-6.625 5.375-12 12-12zM4.547 19.234c0.031-0.063-0.016-0.141-0.109-0.187-0.094-0.031-0.172-0.016-0.203 0.031-0.031 0.063 0.016 0.141 0.109 0.187 0.078 0.047 0.172 0.031 0.203-0.031zM5.031 19.766c0.063-0.047 0.047-0.156-0.031-0.25-0.078-0.078-0.187-0.109-0.25-0.047-0.063 0.047-0.047 0.156 0.031 0.25 0.078 0.078 0.187 0.109 0.25 0.047zM5.5 20.469c0.078-0.063 0.078-0.187 0-0.297-0.063-0.109-0.187-0.156-0.266-0.094-0.078 0.047-0.078 0.172 0 0.281s0.203 0.156 0.266 0.109zM6.156 21.125c0.063-0.063 0.031-0.203-0.063-0.297-0.109-0.109-0.25-0.125-0.313-0.047-0.078 0.063-0.047 0.203 0.063 0.297 0.109 0.109 0.25 0.125 0.313 0.047zM7.047 21.516c0.031-0.094-0.063-0.203-0.203-0.25-0.125-0.031-0.266 0.016-0.297 0.109s0.063 0.203 0.203 0.234c0.125 0.047 0.266 0 0.297-0.094zM8.031 21.594c0-0.109-0.125-0.187-0.266-0.172-0.141 0-0.25 0.078-0.25 0.172 0 0.109 0.109 0.187 0.266 0.172 0.141 0 0.25-0.078 0.25-0.172zM8.937 21.438c-0.016-0.094-0.141-0.156-0.281-0.141-0.141 0.031-0.234 0.125-0.219 0.234 0.016 0.094 0.141 0.156 0.281 0.125s0.234-0.125 0.219-0.219z"></path>
</svg>

    </a>
    <a id="github-download-link"
       class="icon" title="Download" aria-label="Download"
       href="https://github.com/akshaykhadse/DigitalCognitionBook/archive/v.zip">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M19 9h-4V3H9v6H5l7 7 7-7zM5 18v2h14v-2H5z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
    </a>
  

  <a id="subscribe-link"
     class="icon" title="Subscribe" aria-label="Subscribe"
     href="/DigitalCognitionBook/feed.xml">
    <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <circle cx="6.18" cy="17.82" r="2.18"/>
    <path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/>
</svg>
  </a>

  
  
  
  

  

  

  <!-- Optional additional links to insert for icons links -->
</nav>

  <p>
  &copy; 2020.
  <a href="/DigitalCognitionBook/LICENSE.md">MIT License.</a>
</p>

</div>

    <main class="container">
      <header>
  <h1 class="page-title">Chapter 10: Linear Regression</h1>
</header>
<div class="content">
  <p>Linear regression is concerned with modelling the relationship between 
dependent variable and independent variable.</p>

<p>Suppose a company would like to know how much it should spend on the 
commercials to achieve a certain amount of sales. It has the data regarding 
their previous investments for commercials and the resultant sales figures. To 
get a somewhat accurate estimate, the analysts in the company would like to 
understand the reationship between the sales and the investments into the TV 
commercials.</p>

<p>Let’s establish the following notation:</p>
<ul>
  <li>The amount invested into the TV commercials is denoted by \(x\) because it 
is the independent variable here.</li>
  <li>The sales figure during this period is denoted by \(y\) because it is the 
dependent variable.</li>
  <li>The company has data in form of n pairs like \((x_i, y_i)\) where \(i\) is 
the index.</li>
</ul>

<p>Let’s visualize the data via a scatter plot
<img src="/DigitalCognitionBook/assets/images/linear-regression1.png" alt="Scatter Plot: Sales vs TV" />
We can clearly see a linear trend in this plot. If spending on TV commercials 
increases the sales figures seem to increase.</p>

<p>We describe a general linear relationship between \(x\) and \(y\) via the 
following equation:</p>

\[y = \beta_0 + \beta_1 x\]

<p>If we fit the this curve to the data, we get the following line:</p>

\[y = 6.9748 + 0.0554x\]

<p>Let’s plot this line on our data
<img src="/DigitalCognitionBook/assets/images/linear-regression2.png" alt="Linear Regression Fit" />
We can see that the line passes more or less through the middle of the data 
points.</p>

<p>In this example, we had just one value as input i.e. the \(x\) was scalar. 
However, there might be seveal factors which influence the sales figures like</p>
<ul>
  <li>Time of investment</li>
  <li>Time of commercial</li>
  <li>Expenditure on radio commercials</li>
  <li>Expenditure on newspaper commercials</li>
</ul>

<p>In this case, the \(x\) is not a scalar, but a collection of the above 
attributes. The following section will formalize the linear regression problem 
for such cases.</p>
<h2><a name="ch10-SimpleLinearRegression"></a>Simple Linear Regression</h2>
      <p>Suppose we have a dataset \(\mathcal{D}\) which contains examples 
\(&lt;x_1, y_1&gt;\), \(&lt;x_2, y_2&gt;\), \(&lt;x_3, y_3&gt;\), … \(&lt;x_m, y_m&gt;\) where \(m\) 
is the total number of examples.</p>

<p>Each \(x\) is an input (also known as independent variable). And each \(y\) is 
output (also known as dependent or target variable).</p>

<p>\((x, y)\) represents a single example. \((x_i, y_i)\) is the \(j^{th}\) 
example where \(j\) is index to the dataset.</p>

<p>Each \(x_i\) has \(p\) attributes \(\phi_1(x_i)\), \(\phi_2(x_i)\), …, 
\(\phi_p(x_i)\). Then, we define a new \(m \times (p+1)\) matrix \(\phi\) as</p>

\[\phi = 
\begin{bmatrix}
1 &amp; \phi_1(x_1) &amp; \phi_2(x_1) &amp; ... &amp; \phi_p(x_1)\\
1 &amp; \phi_1(x_2) &amp; \phi_2(x_2) &amp; ... &amp; \phi_p(x_2)\\
.\\
.\\
.\\
1 &amp; \phi_1(x_m) &amp; \phi_2(x_m) &amp; ... &amp; \phi_p(x_m)\\
\end{bmatrix}\]

<p>Let’s also define a \(m \times 1\) matrix \(y\) as</p>

\[y = 
\begin{bmatrix}
y_1 \\
y_2 \\
. \\
. \\
. \\
y_m
\end{bmatrix}\]

<p>The general equation of linear regression model is</p>

\[y_i = w_0 + w_1 \phi_1(x_i) + w_2 \phi_2(x_i) + ... + w_p \phi_p(x_i)\]

<p>where \(w_0\) is the bias and \(w_1\), \(w_2\), …, \(w_p\) are weights. We 
can denote this by a vector \(w\) as</p>

\[w = 
\begin{bmatrix}
w_0 \\
w_1 \\
w_2 \\
. \\
. \\
. \\
w_p
\end{bmatrix}\]

<p>Then the above equation in the matrix form can be written as</p>

\[y_i = w_1 ^ T \phi(x_i)\]

<p>where</p>

\[\phi(x_i) =
\begin{bmatrix}
1 \\
\phi_1(x_i) \\
\phi_2(x_i) \\
. \\
. \\
. \\
\phi_p(x_i)
\end{bmatrix}\]

<p>The same equation can be written for all values of \(y\) as</p>

\[y = \phi(x) w\]

<p>The \(y\) is linear combination of coloumns of matrix \(\phi\) because 
\(i^th\) coloumn of \(\phi\) is multiplied by \(w_i\). But this does not mean 
that linear regression is naievely linear. We can inject nonlinearities via 
the features/basis functions \(\phi_i\)s. For example, we could have used a 
function \(\phi(x_i) = x_i^2\) in the TV commercial example. To summarize, 
\(y\) is linear in \(\phi\) and \(w\) but it may or may not be linear with 
respect to \(x\).</p>

<h3 id="regression-problem">Regression Problem</h3>
<p>Now we define the regression problem as:</p>

<p>Determine a function \(f^*\) such that \(f^*(x)\) is the best predictor 
(minimizes the error \(E(f, \mathcal{D})\)) of \(y\) with respect to the data 
\(\mathcal{D}\).</p>

\[f^* = \underset{f \in \mathcal{F}}{\operatorname{argmin}} E(f, \mathcal{D})\]

<p>Here, \(f\) is a function which belongs to a class of functions 
\(\mathcal{F}\). For linear regression, \(\mathcal{F}\) is class of linear 
functions i.e. \(f\) is a linear function.</p>

<p>When we say best predictor, we must have a measure which we use to determine 
goodness of a function. The error function \(E\) is a measure of deviation of 
predicted value \(\hat{y}\) from the observed value \(y\). Smaller the error, 
better the predictior function \(f\). The error \(E\) is a function of \(f\) 
and data \(\mathcal{D}\). A common error function is Sum of Squared Error. 
There are different types of linear regression based on what error function is 
minimized and what classes of fuctions are considered.</p>

<h3 id="linear-regression-problem">Linear Regression Problem</h3>
<p>If we consider the class of linear functions for \(f\), then 
\(f(\phi(x), w) = \phi(x)w\). We can rewrite the above problem as:</p>

<p>Determine parameters \(w\) for the function \(f(\phi(x), w)\) which minimises 
the error function \(E(f(\phi(x), w), \mathcal{D})\).</p>

\[w^* = \underset{w}{\operatorname{argmin}} E(f(\phi(x), w), \mathcal{D})\]

<h3 id="sum-of-squared-error-sse">Sum of Squared Error (SSE)</h3>
<p>The sum of squares error is defined as follows:</p>

\[E(f, \mathcal{D}) = \sum_{\mathcal{D}}^{} (y_i - f(x_i))^2\]

<p>It measures the square of deviation of each example \((x_i, y_i)\) from the 
prediction \(\hat{y_i} = w^T \phi(x_i)\). Thus, it is a function of \(f\) and 
data \(\mathcal{D}\). This also known as Residual Sum of Squares (RSS).</p>

<p>This error is used to find a least square solution to linear regression 
problem.</p>
<h2><a name="ch10-LeastSquaresSolution"></a>Least Squares Solution</h2>
      <p>We can write the sum of squares error in matrix form as</p>

\[E(f(\phi(x), w) = (y - \phi w)^T(y - \phi w)\]

<p>You might have noticed that we used \(\phi\) instead of \(\phi(x)\). We will 
abuse the notations to make the further mathematical treatment simple to 
understand.</p>

<p>Then we can find the parameters \(\hat{w}\)</p>

\[\hat{w} = \underset{w}{(y - \phi w)^T(y - \phi w)}\]

<p>Differentiating \(E\) with respect to \(w\),</p>

\[\frac{\partial{E}}{\partial{w}} = -2 \phi^T (y - \phi w)\]

\[\frac{\partial^2 E}{\partial{w}\partial{w^T}} = \phi^T \phi\]

<p>If \(\phi\) has full column rank, and hence \(\phi^T \phi\) is positive 
definite, we set the first derivative to zero</p>

\[\phi^T (y - \phi w) = 0\]

<p>To obtain the solution</p>

\[\hat{w} = (\phi^T \phi)^{-1} \phi^T y\]
<h2><a name="ch10-MaximumLikelihoodEstimate"></a>Maximum Likelihood Estimate</h2>
      <p>Suresh tosses a coin 1000 times and he observes heads 400 times.</p>

<p>This is a stochastic process that takes discrete values viz. heads (1) or tails 
(0). In such cases, we can calculate the probability of observing a particular 
set of outcomes by making suitable assumptions about the underlying stochastic 
process. For example, in our case, probability of coin landing heads is \(p\) 
and that coin tosses are independent.</p>

<p>Let’s denote the observed outcomes by \(O\) and the set of parameters that 
describe the stochastic process as \(\theta\). Our observed outcome is 400 
heads and 600 tails. The process of tossing a coin is governed only by the 
probability \(p\). Hence, \(\theta = p\). Thus, when we speak of probability 
we want to calculate \(P(O \mid \theta)\). In other words, given specific 
values for \(\theta\), \(P(O \mid \theta)\) is the probability that we would 
observe the outcomes represented by \(O\).</p>

<p>However, when we model a real life stochastic process, we often do not know 
\(\theta\). We simply observe \(O\) and the goal then is to arrive at an 
estimate for \(\theta\) that would be a plausible choice given the observed 
outcomes \(O\). We know that given a value of \(\theta\) the probability of 
observing \(O\) is \(P(O \mid \theta)\). Thus, a ‘natural’ estimation process 
is to choose that value of \(\theta\) that would maximize the probability that 
we would actually observe \(O\). In other words, we find the parameter values 
\(\theta\) that maximize the following function:</p>

\[L(\theta \mid O) = P(O \mid \theta)\]

<p>\(L(\theta \mid O)\) is called the likelihood function. Notice that by 
definition the likelihood function is conditioned on the observed \(O\) and 
that it is a function of the unknown parameters \(\theta\).</p>

<p>Suresh is about to toss the coin again and he would like to know the 
probability of getting a heads in this toss. We can estimate the probability 
of observing heads as</p>

\[P(Heads) = \frac{400}{1000} = 0.4 = \hat(p)\]

<p>However, it is important to note that we do not know the exact probability of 
getting the heads \(p\). \(\hat{p}\) is an estimate that maximises the 
likelihood of observing the outcomes that we already saw. In other words, it 
the value of the parameter which is the most reasonable explantion for our 
observations.</p>

<h3 id="caveat-continuous-random-variables">Caveat: Continuous Random Variables</h3>

<p>If the stochatistc process has a continous random variable as its outcome, 
then the situation is similar with one important difference. We can no longer 
talk about the probability that we observed \(O\) given \(\theta\) because in 
the continuous case \(P(O \mid \theta)=0\). Without getting into technicalities
, the basic idea is as follows:</p>

<p>Denote the probability density function (pdf) associated with the outcomes 
\(O\) as: \(f(O \mid \theta)\). Thus, in the continuous case we estimate 
\(\theta\) given observed outcomes \(O\) by maximizing the following function:</p>

\[L(\theta \mid O)=f(O \mid \theta)\]

<p>Our use of the pdf instead of actual probability intutuively makes sense 
because if we compare probabilities of getting a small set of outcome e.g. 
\(P(300 \leq O \leq 301 \mid \theta)\) instead of a particular outcome e.g. 
\(P(O = 300 \mid \theta)\), then we can see that the probabilities are 
actually proportional to the value of pdf 
\(f((O_{max} + O_{min})/2 \mid \theta)\).</p>

<p>In this situation, we cannot technically assert that we are finding the 
parameter value that maximizes the probability that we observe \(O\) as we 
maximize the PDF associated with the observed outcomes \(O\).</p>
</div>

    </main>

    <!-- Optional footer content -->
<script src="/DigitalCognitionBook/assets/js/prism.js"></script>
<script src="/DigitalCognitionBook/plugins/normalize-whitespace/prism-normalize-whitespace.js></script>
<script type="text/javascript">
Prism.plugins.NormalizeWhitespace.setDefaults({
'remove-trailing': true,
'remove-indent': false,
'left-trim': false,
'right-trim': false,
});
</script>
<script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 90 }
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  </body>
</html>
